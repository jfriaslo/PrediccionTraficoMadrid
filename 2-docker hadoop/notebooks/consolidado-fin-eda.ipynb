{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX29EMoCZFbV"
   },
   "source": [
    "# Carga de ficheros y preprocesamiento de datos\n",
    "\n",
    "### Si se prueba en local usando docker-compose, hay que hacer un chmod 777 -R . sobre la carpeta notebooks, es un volumen compartido para docker.\n",
    "\n",
    "Los datos de tráfico se supone que están en /mnt/volume/cvs_ok. Este volumen se utiliza también para descargar los datos de puntos de medida y los festivos, en GCP hay que crear un filestorage persistente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suprimimos warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n",
      "1.26.0\n",
      "2.3.2\n",
      "3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print (pyspark.__version__)\n",
    "print (np.__version__)\n",
    "print (pd.__version__)\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "1Hmx5mgozNhN",
    "outputId": "be90899d-a891-402c-f3df-6ea41cc0d839"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "DescargarFicheros = False\n",
    "\n",
    "# Activar para volver a descargar los puntos de medición. Los descarga en el volumen compartido\n",
    "# En despliegue de GCP añadir un filestorage y montarlo.\n",
    "\n",
    "\n",
    "def DescargaPuntos():\n",
    "  for q in range(1, 400, 1):\n",
    "    url = \"https://datos.madrid.es/egob/catalogo/202468-\" + str(q) + \"-intensidad-trafico.csv\"\n",
    "    output_file = \"/mnt/volume/puntos/\" + str(q) + \".csv\"\n",
    "    try:\n",
    "      urllib.request.urlretrieve(url, output_file)\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "if (DescargarFicheros):\n",
    "  DescargaPuntos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "w_NOQ27pAC8U",
    "outputId": "d9618627-5dc4-46c3-e937-bd733d547a8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entornos = []\n",
    "#local = ('local[*]','AppCLusterLocal','/Users/Juanma/Desktop/work-consolidado/01-2024.csv')\n",
    "local = ('local[*]','AppCLusterLocal','/media/juanma/spark/datos/01-2024.csv')\n",
    "compose = ('spark://master:7077','AppCLusterTFM','/mnt/volume/cvs_ok/01-2024.csv')\n",
    "entornos.append(local)\n",
    "entornos.append(compose)\n",
    "fichero = ''\n",
    "entornos\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "global spark\n",
    "\n",
    "def ConexionCluster(entorno=0):\n",
    "# jobs: http://localhost:4040/jobs/\n",
    "    global fichero    \n",
    "    srv = entornos[entorno][0]\n",
    "    app = entornos[entorno][1]\n",
    "    fichero = entornos[entorno][2]    \n",
    "    print (srv)\n",
    "    print (app)\n",
    "    print (fichero)\n",
    "    print ('entorno', entorno)\n",
    "    if entorno > 0:        \n",
    "        return SparkSession.builder \\\n",
    "        .appName(app) \\\n",
    "        .master(srv) \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.cores.max\", 16) \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "        # esto ya está en el fichero de configuración de spark, no es neceario\n",
    "        #.config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master:9000\") \\\n",
    "        \n",
    "        #.config(\"spark.local.dir\", \"/mnt/volume/spark-tmp\") \\\n",
    "    else:\n",
    "        return SparkSession.builder \\\n",
    "            .appName(app) \\\n",
    "            .master(srv) \\\n",
    "            .config(\"spark.local.dir\", \"/mnt/volume/spark-tmp\") \\\n",
    "            .getOrCreate()        \n",
    "\n",
    "#    .config(\"spark.pyspark.python\", \"/usr/bin/python3.11\") \\\n",
    "#    .config(\"spark.pyspark.driver.python\", \"/usr/bin/python3.11\") \\\n",
    "\n",
    "def CargaMuestra():\n",
    "    cad = \"file:\" + fichero\n",
    "    print (cad)\n",
    "    return spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"sep\",\";\").option(\"header\",\"true\") \\\n",
    "    .load(cad)\n",
    "\n",
    "\n",
    "\n",
    "'''spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ColabSpark4.0.0\")   \n",
    "    .config(\"spark.driver.memory\", \"12g\")   \n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")  \n",
    "    # .config(\"spark.executor.memory\", \"2g\")\n",
    "    # Add Delta Lake package configuration\n",
    "    #.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n",
    "    #.config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "'''\n",
    "''''''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectamos al cluster y hacemos algunas pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://master:7077\n",
      "AppCLusterTFM\n",
      "/mnt/volume/cvs_ok/01-2024.csv\n",
      "entorno 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 20:24:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/mnt/volume/cvs_ok/01-2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+---------+----------+---------+-----+----+-----+-------------------+\n",
      "|  id|              fecha|tipo_elem|intensidad|ocupacion|carga|vmed|error|periodo_integracion|\n",
      "+----+-------------------+---------+----------+---------+-----+----+-----+-------------------+\n",
      "|1001|2024-01-01 00:00:00|      C30|       300|      1.0|    0|57.0|    N|                  5|\n",
      "|1001|2024-01-01 00:15:00|      C30|       168|      0.0|    0|62.0|    N|                  5|\n",
      "|1001|2024-01-01 00:30:00|      C30|       540|      2.0|    0|61.0|    N|                  5|\n",
      "+----+-------------------+---------+----------+---------+-----+----+-----+-------------------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|ciudad|\n",
      "+---+------+\n",
      "|  1|Málaga|\n",
      "|  2|Madrid|\n",
      "|  3| Cádiz|\n",
      "+---+------+\n",
      "\n",
      "Spark Context Master: spark://master:7077\n",
      "Resultados:\n",
      "[('es', 1), ('docker', 1), ('esto', 1), ('hola', 1), ('spark', 1), ('con', 1), ('una', 1), ('mundo', 1), ('prueba', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark = ConexionCluster(1)\n",
    "df = CargaMuestra()\n",
    "\n",
    "df.show(3)\n",
    "# =========================================\n",
    "# Pruebas...\n",
    "# =========================================\n",
    "df = spark.createDataFrame(\n",
    "    [(1, \"Málaga\"), (2, \"Madrid\"), (3, \"Cádiz\")],\n",
    "    [\"id\", \"ciudad\"]\n",
    ")\n",
    "df.show()\n",
    "\n",
    "\n",
    "print(\"Spark Context Master:\", spark.sparkContext.master)\n",
    "\n",
    "# Test simple: contar palabras\n",
    "rdd = spark.sparkContext.parallelize([\"hola mundo\", \"esto es una prueba\", \"spark con docker\"])\n",
    "word_counts = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Resultados:\")\n",
    "print(word_counts.collect())\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 134217728)  \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)               \n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "Nu88BeRTbiaY",
    "outputId": "4ebab5f8-f704-4ea4-fb05-9157f42fcf20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c91280f73872:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AppCLusterTFM</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9dd0b4be90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de MINST con torchdistributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 10 executor processes\n",
      "[Epoch 1] Batch 0 Loss: 2.3251395225524902                        (0 + 10) / 10]\n",
      "[Epoch 2] Batch 0 Loss: 0.20318341255187988\n",
      "[Epoch 3] Batch 0 Loss: 0.15609514713287354\n",
      "[Epoch 4] Batch 0 Loss: 0.12071896344423294\n",
      "[Epoch 5] Batch 0 Loss: 0.09943787008523941\n",
      "[Epoch 6] Batch 0 Loss: 0.08340492099523544\n",
      "[Epoch 7] Batch 0 Loss: 0.07449784874916077\n",
      "[Epoch 8] Batch 0 Loss: 0.06500767916440964\n",
      "[Epoch 9] Batch 0 Loss: 0.05636418238282204\n",
      "[Epoch 10] Batch 0 Loss: 0.045968152582645416\n",
      "[Epoch 11] Batch 0 Loss: 0.03986917808651924\n",
      "[Epoch 12] Batch 0 Loss: 0.035076674073934555\n",
      "[Epoch 13] Batch 0 Loss: 0.02744406647980213\n",
      "[Epoch 14] Batch 0 Loss: 0.027873048558831215\n",
      "[Epoch 15] Batch 0 Loss: 0.028492441400885582\n",
      "[Epoch 16] Batch 0 Loss: 0.02563989907503128\n",
      "[Epoch 17] Batch 0 Loss: 0.024664783850312233\n",
      "[Epoch 18] Batch 0 Loss: 0.01674218475818634\n",
      "[Epoch 19] Batch 0 Loss: 0.018313443288207054\n",
      "Finished distributed training with 10 executor processes                        \n"
     ]
    }
   ],
   "source": [
    "## Este ejemplo está tomado de github. \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('/tmp/mnist', train=True, download=True, transform=transform)\n",
    "    sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    model = Net()\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 20):  # solo 2 epochs para prueba\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and rank == 0:\n",
    "                print(f\"[Epoch {epoch}] Batch {batch_idx} Loss: {loss.item()}\")\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "distributor = TorchDistributor(\n",
    "    #num_processes=3,  # spark.sparkContext.defaultParallelism\n",
    "    num_processes=spark.sparkContext.defaultParallelism,\n",
    "    local_mode = False,\n",
    "    use_gpu=False)\n",
    "\n",
    "distributor.run(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7z8fRySS5VV",
    "outputId": "5f6ba75d-b726-420f-a984-bab09b939520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.csv  137.csv  173.csv  209.csv  242.csv  37.csv  71.csv\n",
      "104.csv  140.csv  176.csv  212.csv  245.csv  40.csv  74.csv\n",
      "107.csv  143.csv  179.csv  215.csv  248.csv  43.csv  77.csv\n",
      "110.csv  146.csv  182.csv  218.csv  251.csv  46.csv  80.csv\n",
      "113.csv  149.csv  185.csv  221.csv  254.csv  49.csv  83.csv\n",
      "116.csv  152.csv  188.csv  224.csv  257.csv  52.csv  86.csv\n",
      "119.csv  155.csv  191.csv  227.csv  25.csv   55.csv  89.csv\n",
      "122.csv  158.csv  194.csv  22.csv   260.csv  58.csv  92.csv\n",
      "126.csv  161.csv  197.csv  230.csv  263.csv  61.csv  95.csv\n",
      "128.csv  164.csv  200.csv  233.csv  28.csv   64.csv  98.csv\n",
      "131.csv  167.csv  203.csv  236.csv  31.csv   67.csv  festivos2025.csv\n",
      "134.csv  170.csv  206.csv  239.csv  34.csv   68.csv  historico_festivos.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/volume/puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FsgniKIpqj9d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"tipo_elem\", StringType(), True),\n",
    "    StructField(\"distrito\", IntegerType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"cod_cent\", StringType(), True),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"utm_x\", DoubleType(), True),\n",
    "    StructField(\"utm_y\", DoubleType(), True),\n",
    "    StructField(\"longitud\", DoubleType(), True),\n",
    "    StructField(\"latitud\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "acumulado = None\n",
    "\n",
    "li = os.listdir(\"/mnt/volume/puntos\")\n",
    "\n",
    "for q in range(1, 400, 1):\n",
    "    file_path = \"/mnt/volume/puntos/\" + str(q) + \".csv\"\n",
    "    if str(q) + '.csv' in li:\n",
    "      nuevo_df = spark.read.format(\"csv\") \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(schema) \\\n",
    "            .load('file://' + file_path)\n",
    "      if acumulado is None:\n",
    "        acumulado = nuevo_df\n",
    "      else:\n",
    "        existentes = acumulado.join(nuevo_df, on=\"id\", how=\"left_anti\")\n",
    "        acumulado = existentes.unionByName(nuevo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2r5j04A1uTT0",
    "outputId": "c644ecf8-7821-4924-cb0f-c8c9cb19265a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:25:19 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/09/08 20:25:19 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/09/08 20:25:21 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+--------+--------------------+----------------+----------------+-----------------+----------------+\n",
      "|   id|tipo_elem|distrito|cod_cent|              nombre|           utm_x|           utm_y|         longitud|         latitud|\n",
      "+-----+---------+--------+--------+--------------------+----------------+----------------+-----------------+----------------+\n",
      "|10339|      URB|       9|   17034|Quintana E-O - Ma...|439184.091811248|4475516.01939195|-3.71696021836144|40.4280582423154|\n",
      "| 6504|      URB|      20|   71039|Av. Jaca - Av. Ja...|            NULL|            NULL|             NULL|            NULL|\n",
      "| 7132|      URB|       4|    6016|(AFOROS) Velazque...|441954.508259539|4475053.14160329|-3.68425971438312|40.4240863309414|\n",
      "| 4300|      URB|       1|   16025|Mesoneros Romanos...|440247.624601693|4474560.73099691|-3.70433334217253|40.4195296363783|\n",
      "| 3766|      URB|       9|   31061|(TACTICO) Isla de...|438925.304560274|4479395.44576109|-3.72038386410503| 40.462986684537|\n",
      "|10385|      URB|       1|   16010|Gran Via,67 N-S -...|439773.323163234| 4474925.6251193|-3.70995833940009|40.4227825750685|\n",
      "| 4299|      URB|       1|   16023|Gran Via, 39 N-S ...|440158.237773862|4474641.13731443|-3.70539448268993|40.4202475504556|\n",
      "| 4210|      URB|       1|   14014|MESON DE PAREDES ...|440474.620507374|4473073.07985533| -3.7015186222399|40.4061444272758|\n",
      "|10396|      URB|      10|   80305|(MICRO) AV. POBLA...|434605.628779338|4471816.30114856|-3.77055111104685|40.3943826002871|\n",
      "|10399|      URB|      10|   80306|(MICRO)CTRA. BOAD...|435216.777649442|4472420.69089918|-3.76341196064405|40.3998749343603|\n",
      "| 6181|      URB|      15|   53403|C/. Jos� Mar�a Pe...|445061.716676064|4475580.80989117|-3.64767877925482|40.4290508375435|\n",
      "| 6046|      URB|      15|   46401|C/. Arturo Soria ...|443698.536408876|4479237.52144574|-3.66407271268682|40.4619011833726|\n",
      "| 6047|      URB|      15|   46402|C/. Arturo Soria ...|443860.608937419|4479100.92996123|-3.66214911512391|40.4606816679057|\n",
      "| 4368|      URB|       9|   17037|FERRAZ S-N(VENTUR...|439350.291593553|4475031.66944221|-3.71495482705921|40.4237071407905|\n",
      "| 4463|      URB|       7|   22010|(TACTICO) Cristób...|441041.407275647|4477142.59278748|-3.69521674172708|40.4428448679084|\n",
      "| 3682|      URB|      12|   39408|(TACTICO) CRISTO ...|440460.368196928|4470323.33579525|-3.70142948888215|40.3813723076021|\n",
      "| 5830|      URB|      13|   41020|C/. Peña Gorbea -...| 443351.72117136|4471909.14922152|-3.66751007221442|40.3958598235286|\n",
      "| 6145|      URB|      15|   53014|C/. Poeta Blas de...|444371.963997363|4474827.58166048|-3.65574401213085|40.4222195065843|\n",
      "| 6193|      URB|      15|   53415|C/. María Teresa ...| 444441.66871027|4474788.73175433| -3.6549189887983|40.4218741823462|\n",
      "| 6247|      URB|      16|   55030|Av. Machupichu  -...|447134.587787604|4478516.98633109| -3.6234870134731|40.4556359271282|\n",
      "+-----+---------+--------+--------+--------------------+----------------+----------------+-----------------+----------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:25:35 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "[Stage 174:==================================================>    (75 + 7) / 82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "acumulado.show()\n",
    "print(acumulado.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50XWDjPGgUQm",
    "outputId": "603d6b3e-3e03-4498-8364-a395f65b7337"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:25:53 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/09/08 20:25:54 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/09/08 20:25:54 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/09/08 20:25:55 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------+--------+--------------------+-----+-----+--------+-------+\n",
      "|  id|tipo_elem|distrito|cod_cent|              nombre|utm_x|utm_y|longitud|latitud|\n",
      "+----+---------+--------+--------+--------------------+-----+-----+--------+-------+\n",
      "|6504|      URB|      20|   71039|Av. Jaca - Av. Ja...| NULL| NULL|    NULL|   NULL|\n",
      "+----+---------+--------+--------+--------------------+-----+-----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acumulado = acumulado.where('id is not NULL')\n",
    "acumulado.createOrReplaceTempView(\"qqq\")\n",
    "spark.sql('select * from qqq where longitud is null or latitud is null or id is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "Qmah-mEExut9",
    "outputId": "4cd51e48-2f89-4104-a4dc-349b9ab53ead",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:26:09 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "25/09/08 20:26:13 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|count(1)|\n",
      "+---+--------+\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_puntos = acumulado\n",
    "df_puntos.createOrReplaceTempView(\"qqq\")\n",
    "spark.sql('select id,count(*) from qqq where id is not null group by id having count(*)>1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puntos = spark.sql('select id,longitud, latitud from qqq where id is not null and longitud is not null and latitud is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:26:26 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    5174|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:26:40 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "5173"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select count (*) from qqq').show()\n",
    "\n",
    "df_puntos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "qBV72Ebxzqxe",
    "outputId": "e93115aa-b560-4236-e9bb-ee4246e94735",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a�o: integer (nullable = true)\n",
      " |-- fecha_festivo: date (nullable = true)\n",
      " |-- festividad: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url_festivos = 'https://datos.comunidad.madrid/catalogo/dataset/02c712b5-5009-4ffb-b388-3cfb4fca207d/resource/02d9ad68-6614-4105-a0e7-0a3c429859cf/download/festivos_regionales_historicos.csv'\n",
    "url_2025 = 'https://datos.comunidad.madrid/dataset/f160eb6c-6715-471e-9bc0-38497aae950f/resource/eb9fe481-076d-48bb-bfe4-0df0e301f765/download/festivos_regionales.csv'\n",
    "\n",
    "output_file = \"/mnt/volume/puntos/historico_festivos.csv\"\n",
    "output_file25 = \"/mnt/volume/puntos/festivos2025.csv\"\n",
    "urllib.request.urlretrieve(url_festivos, output_file)\n",
    "\n",
    "df_festivos = spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"sep\",\";\").option(\"header\",\"true\") \\\n",
    "    .load(\"file:///mnt/volume/puntos/historico_festivos.csv\")\n",
    "\n",
    "urllib.request.urlretrieve(url_2025, output_file25)\n",
    "df_festivos2 = spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"sep\",\";\").option(\"header\",\"true\") \\\n",
    "    .load(\"file:///mnt/volume/puntos/festivos2025.csv\")\n",
    "\n",
    "df_festivos = df_festivos.union(df_festivos2)\n",
    "\n",
    "df_festivos.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKiZFjy10z3c",
    "outputId": "97c23af3-da83-4e00-8865-f2e85eaf693a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+\n",
      "| a�o|fecha_festivo|          festividad|\n",
      "+----+-------------+--------------------+\n",
      "|1998|   1998-01-01|           A�o Nuevo|\n",
      "|1998|   1998-01-06|  Epifan�a del Se�or|\n",
      "|1998|   1998-03-19|            San Jos�|\n",
      "|1998|   1998-04-09|        Jueves Santo|\n",
      "|1998|   1998-04-10|       Viernes Santo|\n",
      "|1998|   1998-05-01|  Fiesta del Trabajo|\n",
      "|1998|   1998-05-02|Fiesta de la Comu...|\n",
      "|1998|   1998-08-15|Asunci�n de la Vi...|\n",
      "|1998|   1998-10-12|Fiesta Nacional d...|\n",
      "|1998|   1998-11-02|    Todos los Santos|\n",
      "|1998|   1998-12-08|Inmaculada Concep...|\n",
      "|1998|   1998-12-25| Natividad del Se�or|\n",
      "|1999|   1999-01-01|           A�o Nuevo|\n",
      "|1999|   1999-01-06|  Epifan�a del Se�or|\n",
      "|1999|   1999-03-19|            San Jos�|\n",
      "|1999|   1999-04-01|        Jueves Santo|\n",
      "|1999|   1999-04-02|       Viernes Santo|\n",
      "|1999|   1999-05-01|  Fiesta del Trabajo|\n",
      "|1999|   1999-05-15|          San Isidro|\n",
      "|1999|   1999-08-16|Asunci�n de la Vi...|\n",
      "+----+-------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_festivos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONb1Om6N3GIu",
    "outputId": "de16af2a-2439-4bc4-c41e-1a17ccd36c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|fecha_festivo|\n",
      "+-------------+\n",
      "|   2000-04-20|\n",
      "|   2000-04-21|\n",
      "|   2000-05-15|\n",
      "|   2001-01-01|\n",
      "|   2001-05-01|\n",
      "|   2001-05-02|\n",
      "|   2001-05-15|\n",
      "|   2002-03-29|\n",
      "|   2002-05-01|\n",
      "|   2003-01-01|\n",
      "|   2004-01-01|\n",
      "|   2005-12-25|\n",
      "|   2006-01-01|\n",
      "|   2006-04-14|\n",
      "|   2006-05-01|\n",
      "|   2006-12-08|\n",
      "|   2008-07-25|\n",
      "|   2009-01-01|\n",
      "|   2009-01-06|\n",
      "|   2009-05-02|\n",
      "+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df_festivos = df_festivos.select(date_format('fecha_festivo','yyyy-MM-dd').alias('fecha_festivo')).distinct()\n",
    "df_festivos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_festivos.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/festivos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:26:52 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_puntos.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:27:07 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "[Stage 688:======================================>               (59 + 10) / 82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------------------+\n",
      "|summary|               id|            longitud|             latitud|\n",
      "+-------+-----------------+--------------------+--------------------+\n",
      "|  count|             5173|                5173|                5173|\n",
      "|   mean|6832.655325729751|  -3.684087021639414|   40.43038444497566|\n",
      "| stddev|2692.607578324742|0.042889279891849236|0.038829711673342614|\n",
      "|    min|                0|   -3.83688562997397|   40.33245364209287|\n",
      "|    max|            11451|   -3.55162282763333|   40.51561092557125|\n",
      "+-------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_puntos.describe().show()\n",
    "df_festivos.createOrReplaceTempView(\"tb_festivos\")\n",
    "df_puntos.createOrReplaceTempView(\"tb_puntos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "XVe_e04iZuui",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## En este punto tenemos en df_puntos los puntos de medida y en df_festivos los días festivos. A continuación, leemos el dataset. También los hemos grabado en hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "'''root\n",
    " |-- id: integer (nullable = true)\n",
    " |-- fecha: timestamp (nullable = true)\n",
    " |-- tipo_elem: string (nullable = true)\n",
    " |-- intensidad: integer (nullable = true)\n",
    " |-- ocupacion: double (nullable = true)\n",
    " |-- carga: integer (nullable = true)\n",
    " |-- vmed: double (nullable = true)\n",
    " |-- error: string (nullable = true)\n",
    " |-- periodo_integracion: integer (nullable = true)\n",
    "'''\n",
    "\n",
    "## Esquema del conjunto de datos\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"fecha\", TimestampType(), True),\n",
    "    StructField(\"tipo_elem\", StringType(), True),\n",
    "    StructField(\"intensidad\", IntegerType(), True),\n",
    "    StructField(\"ocupacion\", DoubleType(), True),\n",
    "    StructField(\"carga\", IntegerType(), True),\n",
    "    StructField(\"vmed\", DoubleType(), True),\n",
    "    StructField(\"error\", StringType(), True),\n",
    "    StructField(\"periodo_integracion\", IntegerType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_char,year, month, dayofmonth, hour, minute, dayofweek, weekofyear, quarter, expr, split, lpad, concat_ws, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "def Preprocesa(my_df):    \n",
    "    my_df = my_df.withColumn(\"year\", year(col(\"fecha\"))) \\\n",
    "      .withColumn(\"month\", month(col(\"fecha\"))) \\\n",
    "      .withColumn(\"day\", dayofmonth(col(\"fecha\"))) \\\n",
    "      .withColumn(\"hour\", hour(col(\"fecha\"))) \\\n",
    "      .withColumn(\"min\", minute(col(\"fecha\"))) \\\n",
    "      .withColumn(\"day_of_week\", expr(\"((dayofweek(fecha) + 5) % 7) + 1\")) \\\n",
    "      .withColumn(\"week_of_year\", weekofyear(col(\"fecha\"))) \\\n",
    "      .withColumn(\"num_min\", hour(col(\"fecha\")) * 60 + minute(col(\"fecha\"))) \\\n",
    "      .withColumn(\"quarter\", quarter(col(\"fecha\")))       \n",
    "    my_df = my_df.withColumn(\"tipo_elem\", when(col(\"tipo_elem\") == \"M30\", \"C30\").otherwise(col(\"tipo_elem\")))\n",
    "    my_df.createOrReplaceTempView(\"tb_temp\")    \n",
    "    joined_df = spark.sql('SELECT m.*,p.longitud,p.latitud from tb_puntos p, tb_temp m WHERE m.id = p.id')\n",
    "    my_df = joined_df    \n",
    "    my_df = my_df.where(\"error = 'N'\")\n",
    "    my_df = my_df.dropna().dropDuplicates().drop('id')\n",
    "    my_df.createOrReplaceTempView(\"tb_temp\")    \n",
    "    my_df = spark.sql('''SELECT m.*,'S' as festivo from tb_temp m\n",
    "        WHERE date_format(fecha,\"yyyy-MM-dd\") in (select date_format(fecha_festivo,\"yyyy-MM-dd\")  from tb_festivos)\n",
    "        union\n",
    "        SELECT m.*,'N' as festivo from tb_temp m \n",
    "        WHERE date_format(fecha,\"yyyy-MM-dd\") not in (select date_format(fecha_festivo,\"yyyy-MM-dd\") from tb_festivos)''')      \n",
    "    my_df = my_df.withColumn('fecha',date_format('fecha','yyyy-MM-dd'))    \n",
    "    my_df = my_df.drop('vmed').drop('error').drop('periodo_integracion').where('intensidad >= 0 and carga >=0')    \n",
    "    return my_df\n",
    "\n",
    "\n",
    "\n",
    "def PreprocesaFechasErroneas(my_df):\n",
    "    split_datetime = split(col(\"fecha\"), \" \")\n",
    "    date_part_str = split_datetime.getItem(0)\n",
    "    time_part_str = split_datetime.getItem(1)\n",
    "    split_date = split(date_part_str, \"/\")\n",
    "    day_part = split_date.getItem(0)\n",
    "    month_part = split_date.getItem(1)\n",
    "    year_part = split_date.getItem(2)\n",
    "    day_part_padded = lpad(day_part, 2, \"0\")\n",
    "    month_part_padded = lpad(month_part, 2, \"0\")\n",
    "    padded_date_part = concat_ws(\"/\", day_part_padded, month_part_padded, year_part)\n",
    "    time_split = split(time_part_str, \":\")\n",
    "    hour_part = time_split.getItem(0)\n",
    "    minute_part = time_split.getItem(1)\n",
    "    hour_part_padded = lpad(hour_part, 2, \"0\")\n",
    "    cleaned_fecha_string = concat_ws(\" \", padded_date_part, concat_ws(\":\", hour_part_padded, minute_part))\n",
    "    my_df = my_df.withColumn(\"fecha_timestamp\", to_timestamp(cleaned_fecha_string, \"dd/MM/yy HH:mm\"))   \n",
    "    my_df = my_df.withColumn(\"tipo_elem\", when(col(\"tipo_elem\") == \"M30\", \"C30\").otherwise(col(\"tipo_elem\")))\n",
    "    my_df = my_df.withColumn(\"year\", year(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"month\", month(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"day\", dayofmonth(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"hour\", hour(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"min\", minute(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"day_of_week\", expr(\"((dayofweek(fecha_timestamp) + 5) % 7) + 1\")) \\\n",
    "      .withColumn(\"week_of_year\", weekofyear(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"num_min\", hour(col(\"fecha_timestamp\")) * 60 + minute(col(\"fecha_timestamp\"))) \\\n",
    "      .withColumn(\"quarter\", quarter(col(\"fecha_timestamp\")))          \n",
    "    my_df = my_df.withColumn('fecha',date_format('fecha_timestamp','yyyy-MM-dd'))\n",
    "    my_df.createOrReplaceTempView(\"tb_temp\")    \n",
    "    joined_df = spark.sql('SELECT m.*,p.longitud,p.latitud from tb_puntos p, tb_temp m WHERE m.id = p.id')\n",
    "    my_df = joined_df    \n",
    "    my_df = my_df.where(\"error = 'N'\")\n",
    "    my_df = my_df.dropna().dropDuplicates().drop('id')\n",
    "    my_df.createOrReplaceTempView(\"tb_temp\")    \n",
    "    my_df = spark.sql('''SELECT m.*,'S' as festivo from tb_temp m\n",
    "        WHERE fecha in (select date_format(fecha_festivo,\"yyyy-MM-dd\")  from tb_festivos)\n",
    "        union\n",
    "        SELECT m.*,'N' as festivo from tb_temp m \n",
    "        WHERE fecha not in (select date_format(fecha_festivo,\"yyyy-MM-dd\") from tb_festivos)''')        \n",
    "    my_df = my_df.drop('id').drop('fec').drop('fecha_timestamp').drop('vmed').drop('error').drop('periodo_integracion').where('intensidad >= 0 and carga >=0')    \n",
    "    return my_df\n",
    "\n",
    "def LeerMes(mm, aaaa):\n",
    "    archivo = '/mnt/volume/cvs_ok/' + aaaa + '/' + mm + '-' + aaaa + '.csv'\n",
    "    print (archivo)\n",
    "    temp =  spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"sep\",\";\").option(\"header\",\"true\").schema(schema).load('file://' + archivo)    \n",
    "    temp = Preprocesa(temp)   \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De 2025, tenemos datos hasta julio.\n",
    "\n",
    "def LeerAAAA(aaaa):\n",
    "    acumulado = None  \n",
    "    top = 13\n",
    "    if  aaaa == '2025':\n",
    "        top = 8\n",
    "    for r in range(1,top):\n",
    "        nn = f\"{r:02d}\"  \n",
    "        if r == 1:\n",
    "            acumulado = LeerMes(nn,aaaa)\n",
    "        else:\n",
    "            try:\n",
    "                k = LeerMes(nn,aaaa)\n",
    "                acumulado = acumulado.union(k)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer archivo {nn} {aaaa}': {e}\")\n",
    "        print (f'{aaaa} {nn}')        \n",
    "    df_puntos.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/{aaaa}\")\n",
    "    return acumulado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/volume/cvs_ok/2025/01-2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:27:47 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 20:27:53 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 20:28:00 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+----------------+-----------------+-------+\n",
      "|     fecha|tipo_elem|intensidad|ocupacion|carga|year|month|day|hour|min|day_of_week|week_of_year|num_min|quarter|        longitud|          latitud|festivo|\n",
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+----------------+-----------------+-------+\n",
      "|2025-01-02|      C30|        60|      1.0|    5|2025|    1|  2|   0| 45|          4|           1|     45|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|        32|      1.0|    4|2025|    1|  2|   1| 30|          4|           1|     90|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|        12|      0.0|    2|2025|    1|  2|   4|  0|          4|           1|    240|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|        24|      0.0|    2|2025|    1|  2|   4| 30|          4|           1|    270|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|        23|      0.0|    2|2025|    1|  2|   4| 45|          4|           1|    285|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|       364|      5.0|   23|2025|    1|  2|   7| 30|          4|           1|    450|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|       503|      6.0|   31|2025|    1|  2|   7| 45|          4|           1|    465|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|       680|      6.0|   41|2025|    1|  2|  14| 45|          4|           1|    885|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|       683|      7.0|   42|2025|    1|  2|  15| 30|          4|           1|    930|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-02|      C30|       724|      7.0|   44|2025|    1|  2|  20| 15|          4|           1|   1215|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|        67|      0.0|    5|2025|    1|  3|   1| 30|          5|           1|     90|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|        24|      0.0|    2|2025|    1|  3|   3| 45|          5|           1|    225|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|        71|      0.0|    5|2025|    1|  3|   6|  0|          5|           1|    360|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       137|      0.0|    8|2025|    1|  3|   6| 30|          5|           1|    390|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       253|      4.0|   16|2025|    1|  3|   7|  0|          5|           1|    420|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       380|      4.0|   25|2025|    1|  3|   7| 30|          5|           1|    450|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       773|      8.0|   48|2025|    1|  3|  11| 30|          5|           1|    690|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       912|      8.0|   55|2025|    1|  3|  12| 30|          5|           1|    750|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       720|      7.0|   44|2025|    1|  3|  15| 30|          5|           1|    930|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "|2025-01-03|      C30|       259|      4.0|   16|2025|    1|  3|  22| 45|          5|           1|   1365|      1|-3.6723088411066|40.48483856745793|      N|\n",
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+----------------+-----------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "my_df = LeerMes('01','2025')     \n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/volume/cvs_ok/2025/02-2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:30:17 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 20:30:29 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 20:30:30 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:30:40 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|     fecha|tipo_elem|intensidad|ocupacion|carga|year|month|day|hour|min|day_of_week|week_of_year|num_min|quarter|         longitud|          latitud|festivo|\n",
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|2025-01-22|      URB|       288|      5.0|   12|2025|    1| 22|   9| 30|          3|           4|    570|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       172|      2.0|    7|2025|    1| 22|   9| 45|          3|           4|    585|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       332|      5.0|   14|2025|    1| 22|  10| 15|          3|           4|    615|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       304|      4.0|   13|2025|    1| 22|  13| 15|          3|           4|    795|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       351|      3.0|   16|2025|    1| 22|  18| 30|          3|           4|   1110|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       244|      2.0|   11|2025|    1| 22|  20|  0|          3|           4|   1200|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       296|      4.0|   13|2025|    1| 22|  20| 30|          3|           4|   1230|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       264|      3.0|   11|2025|    1| 22|  20| 45|          3|           4|   1245|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-22|      URB|       200|      3.0|    9|2025|    1| 22|  22|  0|          3|           4|   1320|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|        37|      1.0|    3|2025|    1| 23|   0| 45|          4|           4|     45|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|        52|      1.0|    3|2025|    1| 23|   1| 30|          4|           4|     90|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|        34|      1.0|    2|2025|    1| 23|   1| 45|          4|           4|    105|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       120|      1.0|    5|2025|    1| 23|   6|  0|          4|           4|    360|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|        64|      1.0|    4|2025|    1| 23|   7|  0|          4|           4|    420|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       168|      1.0|    8|2025|    1| 23|  10| 45|          4|           4|    645|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       248|      3.0|   11|2025|    1| 23|  12| 15|          4|           4|    735|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       248|      2.0|   11|2025|    1| 23|  13| 15|          4|           4|    795|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       313|      4.0|   14|2025|    1| 23|  14|  0|          4|           4|    840|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       213|      2.0|    8|2025|    1| 23|  16| 30|          4|           4|    990|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "|2025-01-23|      URB|       288|      5.0|   14|2025|    1| 23|  19|  0|          4|           4|   1140|      1|-3.67069838875071|40.41137964879736|      N|\n",
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "my_df = my_df.union(LeerMes('02','2025'))\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/volume/cvs_ok/2025/03-2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:35:32 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 20:35:52 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:35:52 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:35:53 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 20:36:08 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|     fecha|tipo_elem|intensidad|ocupacion|carga|year|month|day|hour|min|day_of_week|week_of_year|num_min|quarter|         longitud|          latitud|festivo|\n",
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|2025-01-01|      URB|       241|      2.0|   16|2025|    1|  1|   0| 45|          3|           1|     45|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|       124|      1.0|    9|2025|    1|  1|   3| 15|          3|           1|    195|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|        60|      0.0|    5|2025|    1|  1|   4| 15|          3|           1|    255|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|        72|      0.0|    5|2025|    1|  1|   4| 30|          3|           1|    270|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|        49|      0.0|    5|2025|    1|  1|   8| 15|          3|           1|    495|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|        60|      0.0|    5|2025|    1|  1|   8| 30|          3|           1|    510|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|        90|      1.0|    6|2025|    1|  1|  11| 15|          3|           1|    675|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|       303|      1.0|   21|2025|    1|  1|  18| 30|          3|           1|   1110|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|       211|      2.0|   16|2025|    1|  1|  20|  0|          3|           1|   1200|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|       190|      1.0|   13|2025|    1|  1|  20| 30|          3|           1|   1230|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|       173|      1.0|   13|2025|    1|  1|  21|  0|          3|           1|   1260|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|       127|      1.0|    9|2025|    1|  1|  22| 45|          3|           1|   1365|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-01|      URB|        85|      1.0|    6|2025|    1|  1|  23| 45|          3|           1|   1425|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|       155|      3.0|   12|2025|    1|  6|   0| 15|          1|           2|     15|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|        61|      0.0|    5|2025|    1|  6|   2|  0|          1|           2|    120|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|        18|      1.0|    3|2025|    1|  6|   4| 15|          1|           2|    255|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|        16|      0.0|    1|2025|    1|  6|   5| 45|          1|           2|    345|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|        54|      0.0|    5|2025|    1|  6|   6|  0|          1|           2|    360|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|        78|      1.0|    6|2025|    1|  6|   7| 45|          1|           2|    465|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "|2025-01-06|      URB|       259|      3.0|   18|2025|    1|  6|  18| 30|          1|           2|   1110|      1|-3.75316759460202|40.37240998155409|      S|\n",
      "+----------+---------+----------+---------+-----+----+-----+---+----+---+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "my_df = my_df.union(LeerMes('03','2025'))\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:41:44 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 20:42:04 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:42:04 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:42:04 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 20:42:20 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/09/08 20:42:26 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|count(1)|year|month|\n",
      "+--------+----+-----+\n",
      "|13181133|2025|    1|\n",
      "|13264721|2025|    3|\n",
      "|12012363|2025|    2|\n",
      "+--------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 20:47:34 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 20:47:54 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:47:54 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 20:47:54 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 20:48:08 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|38458217|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Totales Q1 2025.\n",
    "\n",
    "my_df.createOrReplaceTempView(\"datos_q1\")    \n",
    "spark.sql('SELECT count(*),year,month from datos_q1 group by year,month').show()\n",
    "spark.sql('SELECT count(*) from datos_q1').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto realiza la lectura por años en batch. \n",
    "\n",
    "#my_df_2024 = LeerAAAA('2024')\n",
    "#my_df_2025  = LeerAAAA('2025')\n",
    "#my_df_1 = LeerMes('07','2025')                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 21:22:49 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 21:23:08 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:23:08 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:23:09 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:23:22 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/2025Q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/volume/cvs_ok/2025/04-2025.csv\n",
      "/mnt/volume/cvs_ok/2025/05-2025.csv\n",
      "/mnt/volume/cvs_ok/2025/06-2025.csv\n",
      "/mnt/volume/cvs_ok/2025/07-2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 21:38:05 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 21:38:37 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:37 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:37 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:37 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:38 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:38 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:38 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:44 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:38:47 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:38:51 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:38:54 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:38:57 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:39:01 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:39:05 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:39:35 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "my_df_2025 = my_df.union(LeerMes('04','2025'))\n",
    "#my_df_2025.show()\n",
    "my_df_2025 = my_df_2025.union(LeerMes('05','2025'))\n",
    "#my_df_2025.show()\n",
    "my_df_2025 = my_df_2025.union(LeerMes('06','2025'))\n",
    "#my_df_2025.show()\n",
    "my_df_2025 = my_df_2025.union(LeerMes('07','2025'))\n",
    "#my_df_2025.show()\n",
    "\n",
    "my_df_2025.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/2025\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 21:56:17 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 21:56:23 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:23 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:23 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:23 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:25 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:25 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:25 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:25 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:56:49 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:56:59 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:57:03 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:57:12 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:57:17 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 21:57:26 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:57:28 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 21:57:53 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/09/08 21:58:06 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|count(1)|year|month|\n",
      "+--------+----+-----+\n",
      "|12012363|2025|    2|\n",
      "|13440892|2025|    5|\n",
      "|12943951|2025|    6|\n",
      "|13499087|2025|    7|\n",
      "|13181133|2025|    1|\n",
      "|13264721|2025|    3|\n",
      "|12058399|2025|    4|\n",
      "+--------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 22:14:20 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/09/08 22:14:26 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:26 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:26 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:26 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:27 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:28 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:28 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:14:53 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:15:02 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 22:15:06 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 22:15:10 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 22:15:16 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:15:24 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "25/09/08 22:15:31 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/09/08 22:15:54 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|90400546|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df_2025.createOrReplaceTempView(\"datos_2025\")    \n",
    "spark.sql('SELECT count(*),year,month from datos_2025 group by year,month').show()\n",
    "spark.sql('SELECT count(*) from datos_2025').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/volume/cvs_ok/2024/07-2024.csv\n",
      "/mnt/volume/cvs_ok/2024/08-2024.csv\n",
      "/mnt/volume/cvs_ok/2024/09-2024.csv\n",
      "/mnt/volume/cvs_ok/2024/10-2024.csv\n",
      "/mnt/volume/cvs_ok/2024/11-2024.csv\n",
      "/mnt/volume/cvs_ok/2024/12-2024.csv\n"
     ]
    }
   ],
   "source": [
    "# my_df : Q1 2025.\n",
    "# my_df_2025: Año 2025\n",
    "# my_df_aaaa : Datos desde hace un año (julio 24 / julio 25)\n",
    "\n",
    "\n",
    "my_df_aaaa = my_df_2025\n",
    "#my_df_aaaa.show(3)\n",
    "my_df_aaaa= my_df_aaaa.union(LeerMes('07','2024'))\n",
    "#my_df_aaaa.show(3)\n",
    "my_df_aaaa= my_df_aaaa.union(LeerMes('08','2024'))\n",
    "#my_df_aaaa.show(3)\n",
    "my_df_aaaa= my_df_aaaa.union(LeerMes('09','2024'))\n",
    "#my_df_aaaa.show(3)\n",
    "my_df_aaaa= my_df_aaaa.union(LeerMes('10','2024'))\n",
    "#my_df_aaaa.show(3)\n",
    "my_df_aaaa= my_df_aaaa.union(LeerMes('11','2024'))\n",
    "#my_df_aaaa.show(3)\n",
    "my_df_aaaa= my_df_aaaa.union(LeerMes('12','2024'))\n",
    "#my_df_aaaa.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_df_aaaa.createOrReplaceTempView(\"datos_aaaa\")    \n",
    "spark.sql('SELECT count(*),year from datos_aaaa group by year').show()\n",
    "spark.sql('SELECT count(*) from datos_aaaa').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_aaaa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_df_aaaa.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/desdehace1a\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "9UtSmYcFAGur"
   },
   "source": [
    "\n",
    "## Leer dataset\n",
    "\n",
    "# Prueba lectura\n",
    "if (False):\n",
    "    my_df = spark.read.csv(\n",
    "        \"file:///mnt/volume/cvs_ok\", \n",
    "        header=True,           \n",
    "        sep = \";\",\n",
    "        schema =schema     \n",
    "    )\n",
    "else:\n",
    "    #my_df =  spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"sep\",\";\").option(\"header\",\"true\").schema(schema).load('file:/mnt/volume/cvs_ok/01-2024.csv')\n",
    "    my_df = LeerMes('01','2024')\n",
    "    my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_q1 = my_df\n",
    "df_mediciones = my_df\n",
    "#df_mediciones = my_df_aaaa\n",
    "#my_df = df_mediciones\n",
    "my_df.summary().show()\n",
    "#my_df_q1.summary().show()\n",
    "#my_df_2025.summary().show()\n",
    "#my_df_aaaa.summary().show()\n",
    "#df_mediciones.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Primer Q 2025', my_df.count())\n",
    "print ('Primer Q 2025', df_mediciones.count())\n",
    "print ('Primer Q 2025', my_df_q1.count())\n",
    "print ('Año 2025', my_df_2025.count())\n",
    "print ('Ultimo año', my_df_aaaa.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-kQ3BG6qonx",
    "outputId": "6345eb96-079c-4800-eefc-8506dcd18d8a"
   },
   "outputs": [],
   "source": [
    "print(\"DF creados:\")\n",
    "spark_dataframes = {name: obj for name, obj in globals().items() if isinstance(obj, type(spark.createDataFrame([], \"struct<>\")))}\n",
    "for name, zz in spark_dataframes.items():\n",
    "    print(f\"- {name}: {zz}\")\n",
    "\n",
    "print(\"\\nVistas temporales:\")\n",
    "for view_name in spark.catalog.listTables():\n",
    "    if view_name.isTemporary:\n",
    "        print(f\"- {view_name.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = None\n",
    "zz = None\n",
    "acumulado = None\n",
    "nuevo_df = None\n",
    "existentes = None\n",
    "af = None\n",
    "tb_festivos = None\n",
    "joined_df = None\n",
    "tempo = None\n",
    "result = None\n",
    "\n",
    "spark.catalog.dropTempView(\"qqq\")\n",
    "spark.catalog.dropTempView(\"tb_puntos\")\n",
    "spark.catalog.dropTempView(\"tb_medidas\")\n",
    "spark.catalog.dropTempView(\"tb_festivos\")\n",
    "spark.catalog.dropTempView(\"r\")\n",
    "spark.catalog.dropTempView(\"tb_temp\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmTrHGg2Z83X"
   },
   "source": [
    "## Análisis exploratorio de datos. \n",
    "### En este punto, tenemos eliminados, valores nulos y medidas erróneas. Sólo tenemos dos columnas categóricas, tipo_elem y festivo. Los datos los tenemos en df_mediciones (último año). Los pasamos a my_df. Vamos a hacer el estudio sobre el último año (de julio de 2024 a julio de 2025). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqCllHFre5Ni",
    "outputId": "86fb4b46-3c54-4a9b-c02b-6fe2a85910ee"
   },
   "outputs": [],
   "source": [
    "df_mediciones.xdescribe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmy_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:705\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    707\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "my_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb6ef409",
    "outputId": "8a69bdc9-1f5d-4916-b36e-8a199cb3ab26"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "print(\"Comprobamos valores nulos (no debería haberlos):\")\n",
    "my_df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in my_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce6c0e17",
    "outputId": "f71750c1-510f-4e6a-8c4a-60add3051899"
   },
   "source": [
    "## Estudio de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, mean, stddev, corr, percentile_approx\n",
    "\n",
    "numerical_cols = [col for col, dtype in my_df.dtypes if dtype != 'string' and dtype != 'timestamp' and name != 'festivo'] \n",
    "\n",
    "print(\"\\nCorrelación de columnas numéricas con 'intensidad':\")\n",
    "num_cols_cor = [col for col in numerical_cols if col != 'intensidad']\n",
    "\n",
    "print (\"Columnas {num_cols_cor}\")\n",
    "\n",
    "correlaciones = []\n",
    "for col_name in num_cols_cor:\n",
    "    try:\n",
    "        corr = my_df.stat.corr(col_name, 'intensidad')\n",
    "        print(f\"Correlación entre '{col_name}' y 'intensidad': {corr}\")\n",
    "        correlaciones.append((col_name, corr))\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo calcular la correlación para '{col_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Correlaciones: \",correlaciones)\n",
    "\n",
    "\n",
    "for a in correlaciones:    \n",
    "    print(a[0],a[1])\n",
    "    \n",
    "print(\"\\nDescripción de las no numéricas:\")\n",
    "categorical_cols = ['tipo_elem','festivo']\n",
    "\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"\\nRecuento de valores '{col_name}':\")\n",
    "    my_df.groupBy(col_name).count().orderBy(\"count\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e39d9c0a",
    "outputId": "72bc0daa-8092-4ae6-b24c-eb029287cbca"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col, percentile_approx\n",
    "\n",
    "print(\"Estadísticas 'intensidad':\")\n",
    "my_df.select(\n",
    "    mean(\"intensidad\").alias(\"mean_intensidad\"),\n",
    "    stddev(\"intensidad\").alias(\"stddev_intensidad\"),\n",
    "    percentile_approx(\"intensidad\", 0.5, 100).alias(\"median_intensidad\") \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "df890dcb",
    "outputId": "5954526c-12e3-4d9a-98cc-4bb565f93c7e"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, mean, stddev, corr, percentile_approx\n",
    "\n",
    "print(\"Distribución de las columas de categorías:\")\n",
    "categorical_cols = [col_name for col_name, dtype in my_df.dtypes if dtype == 'string']\n",
    "\n",
    "print(\"\\nDistribución de las columnas numéricas:\")\n",
    "numerical_cols = [col_name for col_name, dtype in my_df.dtypes if dtype in ['int', 'double', 'float', 'long']]\n",
    "numerical_cols.remove('intensidad')\n",
    "my_df.select(numerical_cols).describe().show()\n",
    "\n",
    "print(\"\\nMedia y mediana de 'intensidad':\")\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"\\nDatos para '{col_name}':\")\n",
    "    my_df.groupBy(col_name).agg(\n",
    "        mean(\"intensidad\").alias(\"mean_intensidad\"),\n",
    "        percentile_approx(\"intensidad\", 0.5, 100).alias(\"median_intensidad\")\n",
    "    ).orderBy(col_name).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in correlaciones:\n",
    "  print ('Correlación de ' +  t[0] + '      :', t[1])\n",
    "\n",
    "print(\"\\nDescripción de las no numéricas:\")\n",
    "categorical_cols = ['tipo_elem','festivo']\n",
    "my_df.select(categorical_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prueba\n",
    "# my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/2025Q1\")\n",
    "#my_df = spark.read.parquet(\"hdfs://master:9000/medidas/2025Q1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.cache()\n",
    "my_df.createOrReplaceTempView(\"my_df\")\n",
    "spark.sql('select count(*) from my_df').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/2025Q1_integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.createOrReplaceTempView(\"tabla_intensidad\")    \n",
    "q = spark.sql('select latitud, longitud, sum(intensidad) as suma,avg(intensidad) as media from tabla_intensidad group by latitud, longitud')    \n",
    "q.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = q.toPandas()\n",
    "\n",
    "r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r.plot(kind=\"scatter\", x=\"longitud\", y=\"latitud\", grid=True,\n",
    "             s=r[\"suma\"] / 200000, label=\"intensidad\",\n",
    "             c=\"suma\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False, figsize=(10, 7))\n",
    "\n",
    "r.plot(kind=\"scatter\", x=\"longitud\", y=\"latitud\", grid=True,\n",
    "             s=r[\"media\"] / 500, label=\"intensidad\",\n",
    "             c=\"media\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False, figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "my_df.printSchema()\n",
    "columns_with_types = my_df.dtypes\n",
    "integer_cols = [name for name, dtype in columns_with_types if dtype == 'int']\n",
    "\n",
    "for col_name in integer_cols:\n",
    "    my_df = my_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "my_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabamos los ficheros de datos:\n",
    "\n",
    "my_df.cache()\n",
    "my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/my_df_double\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
