{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX29EMoCZFbV"
   },
   "source": [
    "# Modelos de aprendizaje automático\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n",
      "1.26.0\n",
      "2.3.2\n",
      "3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print (pyspark.__version__)\n",
    "print (np.__version__)\n",
    "print (pd.__version__)\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "w_NOQ27pAC8U",
    "outputId": "d9618627-5dc4-46c3-e937-bd733d547a8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entornos = []\n",
    "#local = ('local[*]','AppCLusterLocal','/Users/Juanma/Desktop/work-consolidado/01-2024.csv')\n",
    "local = ('local[*]','AppCLusterLocal','/media/juanma/spark/datos/01-2024.csv')\n",
    "compose = ('spark://master:7077','AppCLusterTFM','/mnt/volume/cvs_ok/01-2024.csv')\n",
    "entornos.append(local)\n",
    "entornos.append(compose)\n",
    "fichero = ''\n",
    "entornos\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "global spark\n",
    "\n",
    "def ConexionCluster(entorno=0):\n",
    "# jobs: http://localhost:4040/jobs/\n",
    "    global fichero    \n",
    "    srv = entornos[entorno][0]\n",
    "    app = entornos[entorno][1]\n",
    "    fichero = entornos[entorno][2]    \n",
    "    print (srv)\n",
    "    print (app)\n",
    "    print (fichero)\n",
    "    print ('entorno', entorno)\n",
    "    if entorno > 0:        \n",
    "        #.config(\"spark.executor.memory\", \"16g\") \\        \n",
    "        return SparkSession.builder \\\n",
    "        .appName(app) \\\n",
    "        .master(srv) \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.cores.max\", 10) \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master:9000\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "        #.config(\"spark.local.dir\", \"/mnt/volume/spark-tmp\") \\\n",
    "    else:\n",
    "        return SparkSession.builder \\\n",
    "            .appName(app) \\\n",
    "            .master(srv) \\\n",
    "            .config(\"spark.local.dir\", \"/mnt/volume/spark-tmp\") \\\n",
    "            .getOrCreate()        \n",
    "\n",
    "#    .config(\"spark.pyspark.python\", \"/usr/bin/python3.11\") \\\n",
    "#    .config(\"spark.pyspark.driver.python\", \"/usr/bin/python3.11\") \\\n",
    "\n",
    "def CargaMuestra():\n",
    "    cad = \"file:\" + fichero\n",
    "    print (cad)\n",
    "    return spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"sep\",\";\").option(\"header\",\"true\") \\\n",
    "    .load(cad)\n",
    "\n",
    "\n",
    "\n",
    "'''spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ColabSpark4.0.0\")\n",
    "    # Increase driver memory (e.g., to 2g or more depending on your data size)\n",
    "    .config(\"spark.driver.memory\", \"12g\")   # increase driver memory\n",
    "    #.config(\"spark.executor.memory\", \"8g\") # increase executor memory\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")  # avoid huge result collection\n",
    "    # Increase executor memory if you have executors (not applicable in local mode usually)\n",
    "    # .config(\"spark.executor.memory\", \"2g\")\n",
    "    # Add Delta Lake package configuration\n",
    "    #.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n",
    "    #.config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "'''\n",
    "''''''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectamos al cluster y hacemos algunas pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://master:7077\n",
      "AppCLusterTFM\n",
      "/mnt/volume/cvs_ok/01-2024.csv\n",
      "entorno 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 23:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|ciudad|\n",
      "+---+------+\n",
      "|  1|Málaga|\n",
      "|  2|Madrid|\n",
      "|  3| Cádiz|\n",
      "+---+------+\n",
      "\n",
      "Spark Context Master: spark://master:7077\n",
      "Resultados:\n",
      "[('es', 1), ('docker', 1), ('esto', 1), ('hola', 1), ('spark', 1), ('con', 1), ('una', 1), ('mundo', 1), ('prueba', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark = ConexionCluster(1)\n",
    "\n",
    "# =========================================\n",
    "# Pruebas...\n",
    "# =========================================\n",
    "df = spark.createDataFrame(\n",
    "    [(1, \"Málaga\"), (2, \"Madrid\"), (3, \"Cádiz\")],\n",
    "    [\"id\", \"ciudad\"]\n",
    ")\n",
    "df.show()\n",
    "\n",
    "\n",
    "print(\"Spark Context Master:\", spark.sparkContext.master)\n",
    "\n",
    "# Test simple: contar palabras\n",
    "rdd = spark.sparkContext.parallelize([\"hola mundo\", \"esto es una prueba\", \"spark con docker\"])\n",
    "word_counts = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Resultados:\")\n",
    "print(word_counts.collect())\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 134217728)  \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)               \n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "Nu88BeRTbiaY",
    "outputId": "4ebab5f8-f704-4ea4-fb05-9157f42fcf20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c91280f73872:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AppCLusterTFM</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72b84f94d990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de MINST con torchdistributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started distributed training with 10 executor processes\n",
      "[Epoch 1] Batch 0 Loss: 2.3251395225524902                        (0 + 10) / 10]\n",
      "[Epoch 2] Batch 0 Loss: 0.20318341255187988\n",
      "[Epoch 3] Batch 0 Loss: 0.15609514713287354\n",
      "[Epoch 4] Batch 0 Loss: 0.12071896344423294\n",
      "[Epoch 5] Batch 0 Loss: 0.09943787008523941\n",
      "[Epoch 6] Batch 0 Loss: 0.08340492099523544\n",
      "[Epoch 7] Batch 0 Loss: 0.07449784874916077\n",
      "[Epoch 8] Batch 0 Loss: 0.06500767916440964\n",
      "[Epoch 9] Batch 0 Loss: 0.05636418238282204\n",
      "[Epoch 10] Batch 0 Loss: 0.045968152582645416\n",
      "[Epoch 11] Batch 0 Loss: 0.03986917808651924\n",
      "[Epoch 12] Batch 0 Loss: 0.035076674073934555\n",
      "[Epoch 13] Batch 0 Loss: 0.02744406647980213\n",
      "[Epoch 14] Batch 0 Loss: 0.027873048558831215\n",
      "[Epoch 15] Batch 0 Loss: 0.028492441400885582\n",
      "[Epoch 16] Batch 0 Loss: 0.02563989907503128\n",
      "[Epoch 17] Batch 0 Loss: 0.024664783850312233\n",
      "[Epoch 18] Batch 0 Loss: 0.01674218475818634\n",
      "[Epoch 19] Batch 0 Loss: 0.018313443288207054\n",
      "Finished distributed training with 10 executor processes                        \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend=\"gloo\")\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('/tmp/mnist', train=True, download=True, transform=transform)\n",
    "    sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    model = Net()\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 20):  # solo 2 epochs para prueba\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and rank == 0:\n",
    "                print(f\"[Epoch {epoch}] Batch {batch_idx} Loss: {loss.item()}\")\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "distributor = TorchDistributor(\n",
    "    #num_processes=3,  # spark.sparkContext.defaultParallelism\n",
    "    num_processes=spark.sparkContext.defaultParallelism,\n",
    "    local_mode = False,\n",
    "    use_gpu=False)\n",
    "\n",
    "distributor.run(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/medidas/2025Q1\")\n",
    "my_df = spark.read.parquet(\"hdfs://master:9000/medidas/2025Q1\")\n",
    "#my_df = spark.read.parquet(\"file:///mnt/volume/medidas_2025Q1\")\n",
    "#my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"file:///mnt/volume/medidas_2025Q1\")\n",
    "#my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"file:///mnt/volume/medidas_2025Q1_double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- tipo_elem: string (nullable = true)\n",
      " |-- intensidad: integer (nullable = true)\n",
      " |-- ocupacion: double (nullable = true)\n",
      " |-- carga: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- min: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- num_min: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- festivo: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- tipo_elem: string (nullable = true)\n",
      " |-- intensidad: double (nullable = true)\n",
      " |-- ocupacion: double (nullable = true)\n",
      " |-- carga: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- hour: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- day_of_week: double (nullable = true)\n",
      " |-- week_of_year: double (nullable = true)\n",
      " |-- num_min: double (nullable = true)\n",
      " |-- quarter: double (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- festivo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "my_df.printSchema()\n",
    "columns_with_types = my_df.dtypes\n",
    "integer_cols = [name for name, dtype in columns_with_types if dtype == 'int']\n",
    "\n",
    "for col_name in integer_cols:\n",
    "    my_df = my_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "my_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+-----+------+-----+----+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|     fecha|tipo_elem|intensidad|ocupacion|carga|  year|month| day|hour| min|day_of_week|week_of_year|num_min|quarter|         longitud|          latitud|festivo|\n",
      "+----------+---------+----------+---------+-----+------+-----+----+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|2025-03-27|      URB|     117.0|      4.0| 29.0|2025.0|  3.0|27.0| 9.0|45.0|        4.0|        13.0|  585.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|      75.0|      7.0| 24.0|2025.0|  3.0|27.0|11.0|45.0|        4.0|        13.0|  705.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|      50.0|      4.0| 14.0|2025.0|  3.0|27.0|12.0| 0.0|        4.0|        13.0|  720.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|     116.0|     10.0| 33.0|2025.0|  3.0|27.0|13.0|15.0|        4.0|        13.0|  795.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|     144.0|      4.0| 27.0|2025.0|  3.0|27.0|14.0|45.0|        4.0|        13.0|  885.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|      79.0|      2.0| 21.0|2025.0|  3.0|27.0|15.0|15.0|        4.0|        13.0|  915.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|      91.0|      5.0| 23.0|2025.0|  3.0|27.0|16.0|45.0|        4.0|        13.0| 1005.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|     185.0|      9.0| 46.0|2025.0|  3.0|27.0|17.0|30.0|        4.0|        13.0| 1050.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|     147.0|      7.0| 32.0|2025.0|  3.0|27.0|20.0|15.0|        4.0|        13.0| 1215.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-27|      URB|      41.0|      1.0|  7.0|2025.0|  3.0|27.0|23.0|30.0|        4.0|        13.0| 1410.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|      37.0|      0.0|  9.0|2025.0|  3.0|28.0| 0.0| 0.0|        5.0|        13.0|    0.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|       7.0|      0.0|  2.0|2025.0|  3.0|28.0| 3.0|30.0|        5.0|        13.0|  210.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|       0.0|      0.0|  0.0|2025.0|  3.0|28.0| 4.0|45.0|        5.0|        13.0|  285.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|      14.0|      0.0|  3.0|2025.0|  3.0|28.0| 5.0|15.0|        5.0|        13.0|  315.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|      12.0|      0.0|  2.0|2025.0|  3.0|28.0| 5.0|30.0|        5.0|        13.0|  330.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|       0.0|      0.0|  0.0|2025.0|  3.0|28.0| 6.0| 0.0|        5.0|        13.0|  360.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|      40.0|      0.0|  7.0|2025.0|  3.0|28.0| 6.0|30.0|        5.0|        13.0|  390.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|      31.0|      1.0|  5.0|2025.0|  3.0|28.0| 6.0|45.0|        5.0|        13.0|  405.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|     102.0|      1.0| 27.0|2025.0|  3.0|28.0|10.0| 0.0|        5.0|        13.0|  600.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "|2025-03-28|      URB|     109.0|      4.0| 25.0|2025.0|  3.0|28.0|12.0| 0.0|        5.0|        13.0|  720.0|    1.0|-3.65341910212066|40.41227295169022|      N|\n",
      "+----------+---------+----------+---------+-----+------+-----+----+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Grabamos los ficheros de datos:\n",
    "\n",
    "my_df.cache()\n",
    "my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/my_df_double\")\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_df = spark.read.parquet(\"hdfs://master:9000/medidas/2025Q1\")\n",
    "#my_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38458217"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- tipo_elem: string (nullable = true)\n",
      " |-- intensidad: double (nullable = true)\n",
      " |-- ocupacion: double (nullable = true)\n",
      " |-- carga: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- hour: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- day_of_week: double (nullable = true)\n",
      " |-- week_of_year: double (nullable = true)\n",
      " |-- num_min: double (nullable = true)\n",
      " |-- quarter: double (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- festivo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee1673b"
   },
   "source": [
    "# Modelo de aprendizaje automático "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "38458217"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = spark.read.parquet(\"hdfs://master:9000/medidas/2025Q1\")\n",
    "my_df.cache()\n",
    "my_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 23:31:11 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- tipo_elem: string (nullable = true)\n",
      " |-- intensidad: integer (nullable = true)\n",
      " |-- ocupacion: double (nullable = true)\n",
      " |-- carga: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- min: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- num_min: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- festivo: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- tipo_elem: string (nullable = true)\n",
      " |-- intensidad: double (nullable = true)\n",
      " |-- ocupacion: double (nullable = true)\n",
      " |-- carga: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- hour: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- day_of_week: double (nullable = true)\n",
      " |-- week_of_year: double (nullable = true)\n",
      " |-- num_min: double (nullable = true)\n",
      " |-- quarter: double (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- festivo: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "my_df.printSchema()\n",
    "columns_with_types = my_df.dtypes\n",
    "integer_cols = [name for name, dtype in columns_with_types if dtype == 'int']\n",
    "\n",
    "for col_name in integer_cols:\n",
    "    my_df = my_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "my_df.printSchema()\n",
    "\n",
    "\n",
    "my_df.cache()\n",
    "my_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/my_df_double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+-----+------+-----+---+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|     fecha|tipo_elem|intensidad|ocupacion|carga|  year|month|day|hour| min|day_of_week|week_of_year|num_min|quarter|         longitud|          latitud|festivo|\n",
      "+----------+---------+----------+---------+-----+------+-----+---+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|2025-01-02|      C30|     516.0|      2.0|  0.0|2025.0|  1.0|2.0| 0.0|15.0|        4.0|         1.0|   15.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     348.0|      2.0|  0.0|2025.0|  1.0|2.0| 0.0|30.0|        4.0|         1.0|   30.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     216.0|      2.0|  0.0|2025.0|  1.0|2.0| 2.0| 0.0|        4.0|         1.0|  120.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     132.0|      1.0|  0.0|2025.0|  1.0|2.0| 3.0|45.0|        4.0|         1.0|  225.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     108.0|      1.0|  0.0|2025.0|  1.0|2.0| 4.0| 0.0|        4.0|         1.0|  240.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     204.0|      1.0|  0.0|2025.0|  1.0|2.0| 5.0| 0.0|        4.0|         1.0|  300.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2004.0|      8.0|  0.0|2025.0|  1.0|2.0| 6.0|30.0|        4.0|         1.0|  390.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2916.0|     11.0|  0.0|2025.0|  1.0|2.0| 7.0|15.0|        4.0|         1.0|  435.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2808.0|     11.0|  0.0|2025.0|  1.0|2.0|12.0| 0.0|        4.0|         1.0|  720.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2592.0|     11.0|  0.0|2025.0|  1.0|2.0|12.0|45.0|        4.0|         1.0|  765.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    3456.0|     13.0|  0.0|2025.0|  1.0|2.0|14.0| 0.0|        4.0|         1.0|  840.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2796.0|     13.0|  0.0|2025.0|  1.0|2.0|15.0| 0.0|        4.0|         1.0|  900.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2736.0|     12.0|  0.0|2025.0|  1.0|2.0|15.0|30.0|        4.0|         1.0|  930.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    3048.0|     12.0|  0.0|2025.0|  1.0|2.0|17.0|45.0|        4.0|         1.0| 1065.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2700.0|     12.0|  0.0|2025.0|  1.0|2.0|18.0|45.0|        4.0|         1.0| 1125.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    1212.0|      6.0|  0.0|2025.0|  1.0|2.0|22.0|30.0|        4.0|         1.0| 1350.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     816.0|      3.0|  0.0|2025.0|  1.0|2.0|23.0|15.0|        4.0|         1.0| 1395.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     528.0|      2.0|  0.0|2025.0|  1.0|2.0|23.0|45.0|        4.0|         1.0| 1425.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-03|      C30|     756.0|      3.0|  0.0|2025.0|  1.0|3.0| 0.0| 0.0|        5.0|         1.0|    0.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-03|      C30|     300.0|      2.0|  0.0|2025.0|  1.0|3.0| 1.0| 0.0|        5.0|         1.0|   60.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "+----------+---------+----------+---------+-----+------+-----+---+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13181133"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = my_df.filter('month = 1')\n",
    "my_df.show()\n",
    "my_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+-----+------+-----+---+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|     fecha|tipo_elem|intensidad|ocupacion|carga|  year|month|day|hour| min|day_of_week|week_of_year|num_min|quarter|         longitud|          latitud|festivo|\n",
      "+----------+---------+----------+---------+-----+------+-----+---+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "|2025-01-02|      C30|     516.0|      2.0|  0.0|2025.0|  1.0|2.0| 0.0|15.0|        4.0|         1.0|   15.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     348.0|      2.0|  0.0|2025.0|  1.0|2.0| 0.0|30.0|        4.0|         1.0|   30.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     216.0|      2.0|  0.0|2025.0|  1.0|2.0| 2.0| 0.0|        4.0|         1.0|  120.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     132.0|      1.0|  0.0|2025.0|  1.0|2.0| 3.0|45.0|        4.0|         1.0|  225.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     108.0|      1.0|  0.0|2025.0|  1.0|2.0| 4.0| 0.0|        4.0|         1.0|  240.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     204.0|      1.0|  0.0|2025.0|  1.0|2.0| 5.0| 0.0|        4.0|         1.0|  300.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2004.0|      8.0|  0.0|2025.0|  1.0|2.0| 6.0|30.0|        4.0|         1.0|  390.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2916.0|     11.0|  0.0|2025.0|  1.0|2.0| 7.0|15.0|        4.0|         1.0|  435.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2808.0|     11.0|  0.0|2025.0|  1.0|2.0|12.0| 0.0|        4.0|         1.0|  720.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2592.0|     11.0|  0.0|2025.0|  1.0|2.0|12.0|45.0|        4.0|         1.0|  765.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    3456.0|     13.0|  0.0|2025.0|  1.0|2.0|14.0| 0.0|        4.0|         1.0|  840.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2796.0|     13.0|  0.0|2025.0|  1.0|2.0|15.0| 0.0|        4.0|         1.0|  900.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2736.0|     12.0|  0.0|2025.0|  1.0|2.0|15.0|30.0|        4.0|         1.0|  930.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    3048.0|     12.0|  0.0|2025.0|  1.0|2.0|17.0|45.0|        4.0|         1.0| 1065.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    2700.0|     12.0|  0.0|2025.0|  1.0|2.0|18.0|45.0|        4.0|         1.0| 1125.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|    1212.0|      6.0|  0.0|2025.0|  1.0|2.0|22.0|30.0|        4.0|         1.0| 1350.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     816.0|      3.0|  0.0|2025.0|  1.0|2.0|23.0|15.0|        4.0|         1.0| 1395.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-02|      C30|     528.0|      2.0|  0.0|2025.0|  1.0|2.0|23.0|45.0|        4.0|         1.0| 1425.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-03|      C30|     756.0|      3.0|  0.0|2025.0|  1.0|3.0| 0.0| 0.0|        5.0|         1.0|    0.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "|2025-01-03|      C30|     300.0|      2.0|  0.0|2025.0|  1.0|3.0| 1.0| 0.0|        5.0|         1.0|   60.0|    1.0|-3.74683415991325|40.40682430154291|      N|\n",
      "+----------+---------+----------+---------+-----+------+-----+---+----+----+-----------+------------+-------+-------+-----------------+-----------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "my_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separamos el conjunto de datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento:  10544713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================================>                  (8 + 4) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  2636420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Separamos en train y test\n",
    "train_data, test_data = my_df.randomSplit([0.8, 0.2], seed=42) \n",
    "\n",
    "print(\"Entrenamiento: \", train_data.count())\n",
    "print(\"Test: \", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoricas: ['tipo_elem', 'festivo']\n",
      "Numericas: ['year', 'month', 'day', 'hour', 'min', 'day_of_week', 'week_of_year', 'num_min', 'quarter', 'longitud', 'latitud']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "categorical_cols = ['tipo_elem', 'festivo']\n",
    "\n",
    "# Omitimos carga y ocupación\n",
    "numerical_cols = [col for col, dtype in my_df.dtypes if dtype in ['int', 'double', 'float', 'long'] and col not in ['intensidad', 'carga','ocupacion']]\n",
    "\n",
    "print(\"Categoricas:\", categorical_cols)\n",
    "print(\"Numericas:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificamos las columnas categóricas y montamos un vector con los datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 23:31:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of scaled training data:\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- intensidad: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- intensidad: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  222942|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10321771|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   55631|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2580789|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_encoded\") for col in categorical_cols]\n",
    "feature_cols = [col + \"_encoded\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='raw_features')\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "feature_pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "feature_model = feature_pipeline.fit(train_data)\n",
    "\n",
    "train_data_features = feature_model.transform(train_data)\n",
    "test_data_features = feature_model.transform(test_data)\n",
    "\n",
    "scaler_model = scaler.fit(train_data_features)\n",
    "\n",
    "train_data_scaled = scaler_model.transform(train_data_features).select('features', 'intensidad')\n",
    "test_data_scaled = scaler_model.transform(test_data_features).select('features', 'intensidad')\n",
    "\n",
    "train_data_scaled.cache()\n",
    "test_data_scaled.cache()\n",
    "\n",
    "print(\"Schema of scaled training data:\")\n",
    "train_data_scaled.printSchema()\n",
    "train_data_scaled.show(5)\n",
    "\n",
    "test_data_scaled.printSchema()\n",
    "test_data_scaled.show(5)\n",
    "\n",
    "train_data_scaled.createOrReplaceTempView(\"train_scaled\")\n",
    "test_data_scaled.createOrReplaceTempView(\"test_scaled\")\n",
    "spark.sql('select count(*) from train_scaled where intensidad = 0').show()\n",
    "spark.sql('select count(*) from train_scaled where intensidad > 0').show()\n",
    "spark.sql('select count(*) from test_scaled where intensidad = 0').show()\n",
    "spark.sql('select count(*) from test_scaled where intensidad > 0').show()\n",
    "\n",
    "train_data_scaled.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/train_data_scaled\")\n",
    "test_data_scaled.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://master:9000/test_data_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'month', 'day', 'hour', 'min', 'day_of_week', 'week_of_year', 'num_min', 'quarter', 'longitud', 'latitud']\n",
      "['tipo_elem', 'festivo']\n"
     ]
    }
   ],
   "source": [
    "print (numerical_cols)\n",
    "print (categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 23:32:41 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  278573|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|12902560|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df.cache()\n",
    "my_df.createOrReplaceTempView(\"tb_my_df\")\n",
    "spark.sql('select count(*) from tb_my_df where intensidad = 0').show()\n",
    "spark.sql('select count(*) from tb_my_df where intensidad > 0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "|[-3.2008233181527...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/08 23:32:42 WARN CacheManager: Asked to cache already cached data.\n",
      "25/09/08 23:32:42 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data_scaled\n",
    "test_data = test_data_scaled\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "train_data.show()\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo básico de regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='intensidad')\n",
    "pipeline = Pipeline(stages=[lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#lr = LinearRegression(featuresCol='features', labelCol='intensidad')\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"intensidad\", regParam=0.1)\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RMSE) test data: 493.22533042258334\n",
      "R-squared test data: 0.25139544300076\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "pred = model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol='intensidad', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(pred, {evaluator.metricName: \"rmse\"})\n",
    "print(f\"(RMSE) test data: {rmse}\")\n",
    "r2 = evaluator.evaluate(pred, {evaluator.metricName: \"r2\"})\n",
    "print(f\"R-squared test data: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='intensidad',\n",
    "    maxIter=100,       \n",
    "    maxDepth=5,        \n",
    "    stepSize=0.1,    \n",
    "    subsamplingRate=0.8, \n",
    "    minInstancesPerNode=10, \n",
    "    lossType=\"squared\"\n",
    ")\n",
    "gbt_pipeline = Pipeline(stages=[gbt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model_gbt = gbt_pipeline.fit(train_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (gbt): 384.00127502775626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2436:=========================================>             (9 + 3) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (gbt): 0.5464904872985833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "pred = model_gbt.transform(test_data)\n",
    "evaluator_gbt = RegressionEvaluator(labelCol='intensidad', predictionCol='prediction')\n",
    "rmse = evaluator_gbt.evaluate(pred, {evaluator_gbt.metricName: \"rmse\"})\n",
    "print(f\"RMSE (gbt): {rmse}\")\n",
    "r2 = evaluator_gbt.evaluate(pred, {evaluator_gbt.metricName: \"r2\"})\n",
    "print(f\"R-squared (gbt): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 21:07:59 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, intensidad: double]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "+--------------------+----------+\n",
      "|            features|intensidad|\n",
      "+--------------------+----------+\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "|[-3.2008766170360...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "copia_train_data = train_data\n",
    "copia_test_data = test_data\n",
    "train_data.show()\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparamos modelo pytorch regresión simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "\n",
    "train_out = (train_data\n",
    "             .withColumn(\"features_arr\", vector_to_array(col(\"features\")))\n",
    "             .select(\"features_arr\", \"intensidad\"))\n",
    "test_out = (test_data\n",
    "            .withColumn(\"features_arr\", vector_to_array(col(\"features\")))\n",
    "            .select(\"features_arr\", \"intensidad\"))\n",
    "\n",
    "train_path = \"file:///mnt/volume/tempo/train_parquet\"\n",
    "test_path  = \"file:///mnt/volume/tempo/test_parquet\"\n",
    "\n",
    "train_out.write.mode(\"overwrite\").parquet(train_path)\n",
    "test_out.write.mode(\"overwrite\").parquet(test_path)\n",
    "\n",
    "\n",
    "def train_main_fn():\n",
    "    import os, torch, torch.nn as nn, torch.optim as optim\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "    import pyarrow.parquet as pq\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    backend = \"gloo\"   # use \"nccl\" if GPUs are available\n",
    "    dist.init_process_group(backend=backend)\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    TRAIN_PATH = \"file:///mnt/volume/tempo/train_parquet\"\n",
    "    TEST_PATH  = \"file:///mnt/volume/tempo/test_parquet\"\n",
    "    \n",
    "    train_tbl = pq.read_table(TRAIN_PATH)\n",
    "    test_tbl  = pq.read_table(TEST_PATH)\n",
    "\n",
    "    X_train = np.stack(train_tbl[\"features_arr\"].to_pylist()).astype(\"float32\")\n",
    "    y_train = np.asarray(train_tbl[\"intensidad\"].to_pylist(), dtype=\"float32\")\n",
    "    X_test  = np.stack(test_tbl[\"features_arr\"].to_pylist()).astype(\"float32\")\n",
    "    y_test  = np.asarray(test_tbl[\"intensidad\"].to_pylist(), dtype=\"float32\")\n",
    "\n",
    "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "    train_sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    test_sampler  = DistributedSampler(test_ds,  num_replicas=world_size, rank=rank, shuffle=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, sampler=train_sampler, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=64, sampler=test_sampler,  num_workers=0)\n",
    "\n",
    "    class RegressionModel(nn.Module):\n",
    "        def __init__(self, in_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "        def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "    model = RegressionModel(in_dim=X_train.shape[1])\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(25):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += loss.item()\n",
    "        avg_train_loss = total / len(train_loader)\n",
    "        print(f\"[rank {rank}] epoch {epoch} train_loss={avg_train_loss:.4f}\", flush=True)\n",
    "\n",
    "        model.eval()\n",
    "        eval_total = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                pred = model(xb)\n",
    "                eval_total += criterion(pred, yb).item()\n",
    "        avg_test_loss = eval_total / len(test_loader)\n",
    "        print(f\"[rank {rank}] epoch {epoch} test_loss={avg_test_loss:.4f}\", flush=True)\n",
    "\n",
    "    if rank == 0:\n",
    "        os.makedirs(\"/mnt/volume/torch_models\", exist_ok=True)\n",
    "        torch.save(model.module.state_dict(), \"/mnt/volume/torch_models/regression.pt\")\n",
    "        print(\"Grabado modelo\", flush=True)\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "TorchDistributor(\n",
    "    num_processes=3,\n",
    "    local_mode=False,\n",
    "    use_gpu=False\n",
    ").run(train_main_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "train_data.printSchema()\n",
    "train_data.show(5)\n",
    "\n",
    "try:\n",
    "    features_list = train_data.select(\"features\").rdd.map(lambda row: row.features.toArray()).collect()\n",
    "    features_np = np.array(features_list, dtype=np.float32)\n",
    "    labels_list = train_data.select(\"intensidad\").rdd.map(lambda row: row.intensidad).collect()\n",
    "    labels_np = np.array(labels_list, dtype=np.float32).reshape(-1, 1)\n",
    "except Exception as e:\n",
    "    print(f\"Errpr de datos: {e}\")\n",
    "    spark.stop()\n",
    "    raise\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_function(features_np, labels_np):\n",
    "    features_tensor = torch.from_numpy(features_np)\n",
    "    labels_tensor = torch.from_numpy(labels_np)\n",
    "    dataset = torch.utils.data.TensorDataset(features_tensor, labels_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    input_size = features_np.shape[1]\n",
    "    model = RegressionModel(input_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "try:\n",
    "    distributor = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False)\n",
    "    trained_model = distributor.run(train_function, features_np, labels_np)\n",
    "    print(\"Completado. Modelo: \", trained_model)\n",
    "except Exception as e:\n",
    "    print(f\"Excepción en el entremaniento: {e}\")\n",
    "    spark.stop()\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
